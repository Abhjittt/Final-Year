{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV_CVvVIgbsn",
        "outputId": "564fc9d7-9d04-4371-d69e-502ed7ced5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVCuVPh2jOdg",
        "outputId": "08fb0a01-35d1-44c2-c2df-19d47082abc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m630.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NaGdW0nUrfj",
        "outputId": "b4310f2e-880e-460e-d213-db4f39951b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "CUDA Version: 12.1\n",
            "Number of GPUs: 1\n",
            "Device 0: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "is_cuda_available = torch.cuda.is_available()\n",
        "print(f'CUDA Available: {is_cuda_available}')\n",
        "\n",
        "# If CUDA is available, print CUDA version and device details\n",
        "if is_cuda_available:\n",
        "    print(f'CUDA Version: {torch.version.cuda}')\n",
        "    print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f'Device {i}: {torch.cuda.get_device_name(i)}')\n",
        "else:\n",
        "    print('CUDA is not available.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdMcRGCWSxj9",
        "outputId": "1b54e3f8-d039-4567-f70d-eb0739a35a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 101MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 0.4148\n",
            "Epoch [2/100], Loss: 0.3579\n",
            "Epoch [3/100], Loss: 0.3126\n",
            "Epoch [4/100], Loss: 0.2481\n",
            "Epoch [5/100], Loss: 0.2681\n",
            "Epoch [6/100], Loss: 0.4238\n",
            "Epoch [7/100], Loss: 0.2453\n",
            "Epoch [8/100], Loss: 0.3150\n",
            "Epoch [9/100], Loss: 0.4034\n",
            "Epoch [10/100], Loss: 0.3853\n",
            "Epoch [11/100], Loss: 0.3038\n",
            "Epoch [12/100], Loss: 0.4705\n",
            "Epoch [13/100], Loss: 0.2725\n",
            "Epoch [14/100], Loss: 0.4652\n",
            "Epoch [15/100], Loss: 0.3034\n",
            "Epoch [16/100], Loss: 0.5076\n",
            "Epoch [17/100], Loss: 0.2739\n",
            "Epoch [18/100], Loss: 0.2975\n",
            "Epoch [19/100], Loss: 0.2658\n",
            "Epoch [20/100], Loss: 0.2020\n",
            "Epoch [21/100], Loss: 0.2360\n",
            "Epoch [22/100], Loss: 0.3142\n",
            "Epoch [23/100], Loss: 0.3934\n",
            "Epoch [24/100], Loss: 0.2841\n",
            "Epoch [25/100], Loss: 0.2966\n",
            "Epoch [26/100], Loss: 0.5602\n",
            "Epoch [27/100], Loss: 0.2569\n",
            "Epoch [28/100], Loss: 0.3847\n",
            "Epoch [29/100], Loss: 0.3801\n",
            "Epoch [30/100], Loss: 0.4902\n",
            "Epoch [31/100], Loss: 0.2834\n",
            "Epoch [32/100], Loss: 0.4642\n",
            "Epoch [33/100], Loss: 0.3279\n",
            "Epoch [34/100], Loss: 0.3536\n",
            "Epoch [35/100], Loss: 0.2495\n",
            "Epoch [36/100], Loss: 0.4331\n",
            "Epoch [37/100], Loss: 0.2071\n",
            "Epoch [38/100], Loss: 0.4087\n",
            "Epoch [39/100], Loss: 0.3313\n",
            "Epoch [40/100], Loss: 0.3549\n",
            "Epoch [41/100], Loss: 0.2754\n",
            "Epoch [42/100], Loss: 0.3612\n",
            "Epoch [43/100], Loss: 0.5199\n",
            "Epoch [44/100], Loss: 0.2903\n",
            "Epoch [45/100], Loss: 0.3371\n",
            "Epoch [46/100], Loss: 0.2350\n",
            "Epoch [47/100], Loss: 0.2797\n",
            "Epoch [48/100], Loss: 0.2002\n",
            "Epoch [49/100], Loss: 0.1987\n",
            "Epoch [50/100], Loss: 0.3431\n",
            "Epoch [51/100], Loss: 0.4248\n",
            "Epoch [52/100], Loss: 0.3666\n",
            "Epoch [53/100], Loss: 0.3441\n",
            "Epoch [54/100], Loss: 0.3029\n",
            "Epoch [55/100], Loss: 0.3542\n",
            "Epoch [56/100], Loss: 0.3344\n",
            "Epoch [57/100], Loss: 0.3847\n",
            "Epoch [58/100], Loss: 0.2156\n",
            "Epoch [59/100], Loss: 0.2075\n",
            "Epoch [60/100], Loss: 0.5029\n",
            "Epoch [61/100], Loss: 0.2366\n",
            "Epoch [62/100], Loss: 0.4061\n",
            "Epoch [63/100], Loss: 0.4042\n",
            "Epoch [64/100], Loss: 0.2485\n",
            "Epoch [65/100], Loss: 0.1821\n",
            "Epoch [66/100], Loss: 0.2698\n",
            "Epoch [67/100], Loss: 0.3151\n",
            "Epoch [68/100], Loss: 0.4142\n",
            "Epoch [69/100], Loss: 0.4223\n",
            "Epoch [70/100], Loss: 0.3353\n",
            "Epoch [71/100], Loss: 0.3131\n",
            "Epoch [72/100], Loss: 0.3846\n",
            "Epoch [73/100], Loss: 0.2402\n",
            "Epoch [74/100], Loss: 0.2039\n",
            "Epoch [75/100], Loss: 0.2008\n",
            "Epoch [76/100], Loss: 0.3486\n",
            "Epoch [77/100], Loss: 0.3040\n",
            "Epoch [78/100], Loss: 0.3601\n",
            "Epoch [79/100], Loss: 0.3306\n",
            "Epoch [80/100], Loss: 0.4303\n",
            "Epoch [81/100], Loss: 0.4248\n",
            "Epoch [82/100], Loss: 0.4237\n",
            "Epoch [83/100], Loss: 0.2897\n",
            "Epoch [84/100], Loss: 0.2722\n",
            "Epoch [85/100], Loss: 0.3393\n",
            "Epoch [86/100], Loss: 0.4011\n",
            "Epoch [87/100], Loss: 0.2392\n",
            "Epoch [88/100], Loss: 0.2408\n",
            "Epoch [89/100], Loss: 0.2356\n",
            "Epoch [90/100], Loss: 0.3001\n",
            "Epoch [91/100], Loss: 0.4319\n",
            "Epoch [92/100], Loss: 0.3800\n",
            "Epoch [93/100], Loss: 0.2324\n",
            "Epoch [94/100], Loss: 0.3772\n",
            "Epoch [95/100], Loss: 0.3272\n",
            "Epoch [96/100], Loss: 0.1793\n",
            "Epoch [97/100], Loss: 0.3413\n",
            "Epoch [98/100], Loss: 0.3904\n",
            "Epoch [99/100], Loss: 0.4030\n",
            "Epoch [100/100], Loss: 0.4518\n",
            "Test Accuracy: 0.7500\n",
            "Test Precision: 0.7344\n",
            "Test Recall: 0.7500\n",
            "Test F1 Score: 0.7419\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Edge extraction function\n",
        "def edge_extraction(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    return np.stack([edges] * 3, axis=-1)  # Stack edges to create a 3-channel image\n",
        "\n",
        "# Custom dataset with noise in labels\n",
        "class CariesDataset(Dataset):\n",
        "    def __init__(self, img_folder, label_folder, transform=None, noise_level=0.1):\n",
        "        self.img_folder = img_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.image_paths = [f for f in os.listdir(img_folder) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_paths[idx]\n",
        "        image_path = os.path.join(self.img_folder, img_name)\n",
        "        label_path = os.path.join(self.label_folder, img_name)\n",
        "\n",
        "        # Load image and apply edge extraction\n",
        "        image = cv2.imread(image_path)\n",
        "        image = edge_extraction(image)\n",
        "\n",
        "        # Load label and convert to single binary value\n",
        "        label_img = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
        "        label = 1 if np.any(label_img > 0) else 0\n",
        "\n",
        "        # Introduce label noise\n",
        "        if random.random() < self.noise_level:\n",
        "            label = 1 - label  # Flip the label\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data Augmentation and Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # Resize for ViT input\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "full_data = CariesDataset(img_folder='/content/drive/MyDrive/Xray/images_cut',\n",
        "                          label_folder='/content/drive/MyDrive/Xray/labels_cut',\n",
        "                          transform=transform)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(full_data))\n",
        "test_size = len(full_data) - train_size\n",
        "train_data, test_data = random_split(full_data, [train_size, test_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load pre-trained Vision Transformer model\n",
        "model = vit_b_16(pretrained=True)\n",
        "\n",
        "# Replace the last layer of the model for binary classification\n",
        "num_classes = 2\n",
        "model.heads[-1] = nn.Linear(model.heads[-1].in_features, num_classes)  # Adjust output layer for binary classification\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# Metrics\n",
        "accuracy = MulticlassAccuracy(num_classes=2).to(device)\n",
        "precision = MulticlassPrecision(num_classes=2).to(device)\n",
        "recall = MulticlassRecall(num_classes=2).to(device)\n",
        "f1_score = MulticlassF1Score(num_classes=2).to(device)\n",
        "\n",
        "# Training loop with 100 epochs as specified\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/100], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "    #print(f\"Accuracy on training set: {accuracy.compute():.4f}\")\n",
        "\n",
        "# Testing phase\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_accuracy, test_precision, test_recall, test_f1 = 0, 0, 0, 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_accuracy += accuracy(preds, labels)\n",
        "        test_precision += precision(preds, labels)\n",
        "        test_recall += recall(preds, labels)\n",
        "        test_f1 += f1_score(preds, labels)\n",
        "\n",
        "    # Average metrics for the test set\n",
        "    test_accuracy /= len(test_loader)\n",
        "    test_precision /= len(test_loader)\n",
        "    test_recall /= len(test_loader)\n",
        "    test_f1 /= len(test_loader)\n",
        "\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA9xRtLebckU",
        "outputId": "8885fc93-bee4-4382-a799-17bcaec5c6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 94.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 0.4731\n",
            "Epoch [2/100], Loss: 0.3281\n",
            "Epoch [3/100], Loss: 0.3101\n",
            "Epoch [4/100], Loss: 0.3008\n",
            "Epoch [5/100], Loss: 0.4344\n",
            "Epoch [6/100], Loss: 0.2481\n",
            "Epoch [7/100], Loss: 0.3661\n",
            "Epoch [8/100], Loss: 0.2401\n",
            "Epoch [9/100], Loss: 0.4406\n",
            "Epoch [10/100], Loss: 0.3403\n",
            "Epoch [11/100], Loss: 0.2836\n",
            "Epoch [12/100], Loss: 0.3492\n",
            "Epoch [13/100], Loss: 0.3488\n",
            "Epoch [14/100], Loss: 0.2795\n",
            "Epoch [15/100], Loss: 0.3245\n",
            "Epoch [16/100], Loss: 0.2779\n",
            "Epoch [17/100], Loss: 0.3040\n",
            "Epoch [18/100], Loss: 0.3393\n",
            "Epoch [19/100], Loss: 0.2652\n",
            "Epoch [20/100], Loss: 0.3622\n",
            "Epoch [21/100], Loss: 0.2995\n",
            "Epoch [22/100], Loss: 0.3069\n",
            "Epoch [23/100], Loss: 0.2887\n",
            "Epoch [24/100], Loss: 0.2417\n",
            "Epoch [25/100], Loss: 0.4345\n",
            "Epoch [26/100], Loss: 0.2763\n",
            "Epoch [27/100], Loss: 0.2916\n",
            "Epoch [28/100], Loss: 0.2369\n",
            "Epoch [29/100], Loss: 0.4909\n",
            "Epoch [30/100], Loss: 0.1764\n",
            "Epoch [31/100], Loss: 0.3679\n",
            "Epoch [32/100], Loss: 0.3349\n",
            "Epoch [33/100], Loss: 0.2077\n",
            "Epoch [34/100], Loss: 0.2448\n",
            "Epoch [35/100], Loss: 0.1993\n",
            "Epoch [36/100], Loss: 0.2800\n",
            "Epoch [37/100], Loss: 0.1963\n",
            "Epoch [38/100], Loss: 0.3413\n",
            "Epoch [39/100], Loss: 0.3823\n",
            "Epoch [40/100], Loss: 0.4268\n",
            "Epoch [41/100], Loss: 0.3338\n",
            "Epoch [42/100], Loss: 0.3144\n",
            "Epoch [43/100], Loss: 0.4354\n",
            "Epoch [44/100], Loss: 0.2798\n",
            "Epoch [45/100], Loss: 0.4029\n",
            "Epoch [46/100], Loss: 0.4552\n",
            "Epoch [47/100], Loss: 0.3647\n",
            "Epoch [48/100], Loss: 0.3813\n",
            "Epoch [49/100], Loss: 0.1963\n",
            "Epoch [50/100], Loss: 0.4419\n",
            "Epoch [51/100], Loss: 0.2750\n",
            "Epoch [52/100], Loss: 0.2887\n",
            "Epoch [53/100], Loss: 0.3023\n",
            "Epoch [54/100], Loss: 0.2613\n",
            "Epoch [55/100], Loss: 0.3979\n",
            "Epoch [56/100], Loss: 0.3800\n",
            "Epoch [57/100], Loss: 0.2932\n",
            "Epoch [58/100], Loss: 0.2977\n",
            "Epoch [59/100], Loss: 0.3922\n",
            "Epoch [60/100], Loss: 0.3234\n",
            "Epoch [61/100], Loss: 0.3490\n",
            "Epoch [62/100], Loss: 0.3296\n",
            "Epoch [63/100], Loss: 0.3482\n",
            "Epoch [64/100], Loss: 0.3315\n",
            "Epoch [65/100], Loss: 0.3563\n",
            "Epoch [66/100], Loss: 0.2706\n",
            "Epoch [67/100], Loss: 0.4644\n",
            "Epoch [68/100], Loss: 0.3560\n",
            "Epoch [69/100], Loss: 0.2586\n",
            "Epoch [70/100], Loss: 0.2673\n",
            "Epoch [71/100], Loss: 0.3848\n",
            "Epoch [72/100], Loss: 0.3498\n",
            "Epoch [73/100], Loss: 0.3285\n",
            "Epoch [74/100], Loss: 0.2789\n",
            "Epoch [75/100], Loss: 0.3526\n",
            "Epoch [76/100], Loss: 0.3726\n",
            "Epoch [77/100], Loss: 0.3551\n",
            "Epoch [78/100], Loss: 0.2929\n",
            "Epoch [79/100], Loss: 0.3847\n",
            "Epoch [80/100], Loss: 0.3279\n",
            "Epoch [81/100], Loss: 0.4209\n",
            "Epoch [82/100], Loss: 0.4365\n",
            "Epoch [83/100], Loss: 0.3245\n",
            "Epoch [84/100], Loss: 0.3561\n",
            "Epoch [85/100], Loss: 0.4087\n",
            "Epoch [86/100], Loss: 0.3475\n",
            "Epoch [87/100], Loss: 0.4330\n",
            "Epoch [88/100], Loss: 0.3368\n",
            "Epoch [89/100], Loss: 0.2577\n",
            "Epoch [90/100], Loss: 0.2609\n",
            "Epoch [91/100], Loss: 0.3438\n",
            "Epoch [92/100], Loss: 0.1827\n",
            "Epoch [93/100], Loss: 0.4106\n",
            "Epoch [94/100], Loss: 0.3347\n",
            "Epoch [95/100], Loss: 0.2640\n",
            "Epoch [96/100], Loss: 0.2456\n",
            "Epoch [97/100], Loss: 0.3296\n",
            "Epoch [98/100], Loss: 0.2056\n",
            "Epoch [99/100], Loss: 0.2702\n",
            "Epoch [100/100], Loss: 0.4012\n",
            "Test Accuracy: 0.7500\n",
            "Test Precision: 0.7344\n",
            "Test Recall: 0.7500\n",
            "Test F1 Score: 0.7419\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        "from torchvision.models import vgg19  # Change here\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Edge extraction function\n",
        "def edge_extraction(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    return np.stack([edges] * 3, axis=-1)  # Stack edges to create a 3-channel image\n",
        "\n",
        "# Custom dataset with noise in labels\n",
        "class CariesDataset(Dataset):\n",
        "    def __init__(self, img_folder, label_folder, transform=None, noise_level=0.1):\n",
        "        self.img_folder = img_folder\n",
        "        self.label_folder = label_folder\n",
        "        self.image_paths = [f for f in os.listdir(img_folder) if f.endswith('.png')]\n",
        "        self.transform = transform\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_paths[idx]\n",
        "        image_path = os.path.join(self.img_folder, img_name)\n",
        "        label_path = os.path.join(self.label_folder, img_name)\n",
        "\n",
        "        # Load image and apply edge extraction\n",
        "        image = cv2.imread(image_path)\n",
        "        image = edge_extraction(image)\n",
        "\n",
        "        # Load label and convert to single binary value\n",
        "        label_img = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
        "        label = 1 if np.any(label_img > 0) else 0\n",
        "\n",
        "        # Introduce label noise\n",
        "        if random.random() < self.noise_level:\n",
        "            label = 1 - label  # Flip the label\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data Augmentation and Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # Resize for VGG input\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "full_data = CariesDataset(img_folder='/content/drive/MyDrive/Xray/images_cut',\n",
        "                          label_folder='/content/drive/MyDrive/Xray/labels_cut',\n",
        "                          transform=transform)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(full_data))\n",
        "test_size = len(full_data) - train_size\n",
        "train_data, test_data = random_split(full_data, [train_size, test_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load pre-trained VGG19 model\n",
        "model = vgg19(pretrained=True)  # Change here\n",
        "\n",
        "# Replace the last layer of the model for binary classification\n",
        "num_classes = 2\n",
        "model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)  # Adjust output layer for binary classification\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# Metrics\n",
        "accuracy = MulticlassAccuracy(num_classes=2).to(device)\n",
        "precision = MulticlassPrecision(num_classes=2).to(device)\n",
        "recall = MulticlassRecall(num_classes=2).to(device)\n",
        "f1_score = MulticlassF1Score(num_classes=2).to(device)\n",
        "\n",
        "# Training loop with 100 epochs as specified\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/100], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "    #print(f\"Accuracy on training set: {accuracy.compute():.4f}\")\n",
        "\n",
        "# Testing phase\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_accuracy, test_precision, test_recall, test_f1 = 0, 0, 0, 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        test_accuracy += accuracy(preds, labels)\n",
        "        test_precision += precision(preds, labels)\n",
        "        test_recall += recall(preds, labels)\n",
        "        test_f1 += f1_score(preds, labels)\n",
        "\n",
        "    # Average metrics for the test set\n",
        "    test_accuracy /= len(test_loader)\n",
        "    test_precision /= len(test_loader)\n",
        "    test_recall /= len(test_loader)\n",
        "    test_f1 /= len(test_loader)\n",
        "\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3njLS09KYO2y",
        "outputId": "f0bcdcd6-d062-4c9b-f20a-eed6aae9b953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.5.1-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
